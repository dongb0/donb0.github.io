---
layout: post
title: "[db/paper] 2 - F1"
subtitle: 
author: "Dongbo"
header-style: text
mathjax: true
hidden: true
tags:
  - database
  - paper reading
---

在 Spanner 论文中提到 F1 似乎采用的是5份 replica（分布在美国东西海岸）

Q：听上去 Spanner 能基本支持 F1 要求的 1.扩展性 2.可用性 3.一致性 4.SQL查询、索引、hoc query
那么为什么需要在 Spanner 之上实现一个 F1 呢？

呃，几篇文献认为上述四个特性之间是互斥的，但是 F1 说它同时实现了以上特性（做了一些取舍），来，看。

F1 在 Spanner 之上增加的特性：
1.分布式SQL
2.二级索引
3.乐观事务？？？
4.history recording and publishing？？？

---------------

sec2

存储还是在 Spanner 上，但是这里说的，Spanner 从底层的 Colosus File System 取数据时，只会取本地（同一个数据中心）的数据；那么 Spanner 的分布式体现在什么地方？

以及 F1 的作用到底是什么

通过 F1 Slave 来处理分布式SQL查询（如果 query planner 预估增加并发能降低延迟的话）

增/删 Spanner server 会引起数据的重新分片，但对 F1 是透明的；增/删 F1 server 则不会引起数据的变动，因为数据存在 Spanner 上。
而虽然 Spanner 读写事务延迟通常为十几毫秒，但加上 F1 之后，提交延迟还是会达到 50～150ms。 ==> 需要客户端改变使用数据库的策略



Spanner ： 一个 zone 包含多个 Paxos Group，一个 Paxos Group 由若干节点（每个节点对应一份replica）组成，每次提交事务时，将会在目标的 Paoxs Group 内进行共识过程。如果该事务涉及改动多个 Group 上的数据，则改为 2PC 协议进行提交 ==> 延迟增加

新的问题：数据如何切分然后存储到多个 zone 或者 Paxos Group 上的？


--------------

sec 3 Data Model 

F1 中，table 是以层级结构的方式组织起来的。有点类似基类和派生类的继承关系，F1 中的表通过将 parent table 的主键拼接在 child table 的主键之前，如 // TODO //，来表示两个表之间的关联关系。

// TODO：论文中的例子

// TODO：存储上由于这样的组织方式，以及 Spanner 中按照 key 进行顺序存储，因此同一个 primary key（parent） 对应的行会存放在一起，查询某个用户的 AdGroup 时只要通过该用户的主键（CustomerId）进行一次范围查询即可。如果在传统关系型数据库中要实现这样的操作，则需要通过索引来查询，但此时我们无法保证对应的行存放在同一个节点上，查询操作可能会变为在多个节点上执行的分布式查询，会导致延迟的增加。另外更重要的是更新数据时，F1 这样的特性（比如一个用户的数据都存在同一个节点）能够尽量避免使用 2PC，在单节点上进行事务提交延迟明显会更低。论文中提到在 AdWords 的应用场景下，大部分事务的操作都只是更新单个广告商的信息，因此比起传统关系型数据库的 schema 模型，采用上述的层级结构 schema 能使延迟降到更低。

- Protocol Buffer
数据序列化与反序列化的库，可以把结构体序列化之后以二进制的形式存在 Spanner 中

- Index

Q: 本身 Spanner 中有 Index 吗？ 

F1 中将 Index 单独作为一个表存在 Spanner。

有 local 和 global 两种。local 是将 root row  primary key 作为索引的 key，因此索引会与 root row 存在同一个 server 上； global 不用 root row primary key （那你怎么映射？？？，就是一个全局的索引吗），通常比较大，所以会被划分到多个 server 上，这也导致更新事务必然要进行 2PC，造成延迟的增加。论文中暂且说谨慎使用 global index。


sec 4：

Schema Changes

谷歌 AdWords 服务要求所用数据库是高可用的，因此 F1 即使是在进行 schema 修改过程中也要保持服务可用，要实现这个需求，需要考虑：
1.F1 的服务器分布在多个数据中心
2.每个服务器在内存中存放这本地的 schema， // TODO：？？ not practical？
3.在变更 schema 的同时不能阻塞正在执行的查询和事务
4.可用性和延迟不能收 schema changes 的影响。

// TODO： needs update

> 稍微去了解了一下，似乎 MySQL 在修改 schema 时，传统做法是要把表锁住，然后将数据拷贝到新的表中；这个过程肯定会导致服务的不可用的。
// TODO： more

F1 更新 schema 采用异步应用更新的方式，

不过异步更新意味着可能出现两个服务器使用不同的 schema 更新数据的情况（Raft 中更新集群配置也需要考虑类似的问题），文中举了个例子说明可能会出现什么样的异常情况： // TODO 例子


因此 F1 中修改 schema 的步骤设计为：

1. 首先限定整个 F1 的集群中，最多两个 schema 处于活跃状态（修改中）。然后对 schema 添加租期（lease），租期失效的 schema 不能被使用；
2. 将 schema 的变更再划分为不会冲突的子步骤
// TODO：这他喵的居然又是一片新的12页论文，例子里描述的我也看不出来如何避免冲突啊？


sec 5：

- Txn

谷歌认为提供最终一致性的系统需要应用程序开发者开发时编写更多的判断逻辑来处理数据可能过时的情况，因此 F1 提供的是 full transactional consistency（//TODO： 这啥？）

F1 中有 3 种事务：

1. Snapshot Transaction  
  利用 Spanner 提供的 Snapshot Read 完成，通常会从本地的 Spanner Server 中读取安全时间戳之前的数据版本；不过用户也可以指定时间戳来读取更新的数据。Default mode for SQL queries and MapReduce

2. Pessimistic Transaction  
  直接映射到 Spanner 的读写事务（Read Write Transaction），即需要获取锁。// 这样理解正确？

3. Optimistic Transaction  
  不加锁的 Read Phase 和 Write Phase。通过 F1 自动生成的 lock 字段中记录的数据最后修改时间戳来判断是否发生冲突的。每次修改数据时（Pessimistic/Optimisitc Txn）会更新该字段中的时间戳，一个乐观事务要提交时，首先客户端从读取到数据中取出时间戳，发送给 F1 Server；F1 创建一个 Pessimistic Txn 检查相关数据项的时间戳是否发生变动，如果第二次读到的时间戳与第一次不同，则说明发生了冲突，需要终止乐观事务。Default for client  

优点：
- 支持长事务。因为不加锁。场景：等待用户交互。
- retriability。// 不太理解
- 易于处理服务器故障。因为乐观事务的状态都由客户端持有，如果某个服务器故障，客户端可以通过其他服务器提交事务。
- 客户端可以从其他地方（如 MapReduce 任务）获取时间戳，然后将其作为乐观事务的提交时间戳。这样能确认乐观事务提交和上个任务结束这短时间内，没有其他事务修改过对应数据（ // TODO: 意义呢？）

缺点：
- 存在插入幻象（Insertion Phantom）问题。不存在的数据项没有对应时间戳，可通过给 parent table 上锁来避免。

// TODO：事务带时间戳，能解决吗？ 导致问题的主要原因是事务不带时间戳，其他乐观事务可以插入数据而无法检测。这是为了把事务信息全部存放在 Client 的取舍吗？ Client 提交时能给事务带上时间戳信息吗？
- 冲突增多时吞吐量下降。


- 不同粒度的锁

// TODO：教材上的 XS 锁之类的
// 以及 Bustub 的锁实现

F1 默认是通过一个 lock 字段来提供行锁，但用户也可以定义额外的 lock 来实现更细粒度的锁，比如每个 lock 只覆盖一部分字段，甚至可以实现每个字段都单独使用一个锁，以此来实现更高的并发。论文中提供的一个应用场景是用户通过前端系统在修改竞标的价格，而后端系统则同时在更新该广告的点击次数等信息，使用细粒度的锁允许这两个操作能够并发执行。

另外，前面也提到过能够使用 parent table 某个 root row 的锁，来封锁所有 child table 对应的数据项，这这主要用于避免插入幻象的问题。

-----------------------------

sec 6： Change History

// TODO： database trigger，触发器，log changes？

我此前居然从来没有接触过相关的内容/需求，甚至第一反应是：采用日志系统的数据库不是可以在 log 中回放数据历史吗，为什么需要采用别的机制来记录 History 呢？

> 关于为什么不用 Log 来记录数据变更历史的思考：  
我认为一方面是日志会定期清除，很难确定在某个位置截断的日志对应多少个数据版本，而近期没有修改的数据项完全没出现在日志中，无法获取历史版本；另一方面是要通过日志重做来获取历史版本，成本太高。

检索得到有参考价值的搜索结果不多。中文搜到看起来比较有意义的来自知乎的提问：[「知乎问题修改记录的数据库是怎样设计的」][6.1]，可以看作是一个需求或者应用场景吧。几个回答提到其他类似功能的社区都是用另一个表存储每个版本的完整记录，需要在网页上对比版本的不同之处时，再通过前端比较和渲染出来的。也就是说数据库本身并没有支持记录数据版本变更，有相关需求的话要在应用内自行编码实现。

英文检索基本上看到的都是 SQLServer 的 Change Tracking & Change Data Capture[^6-2]，看介绍大体也是把修改的数据存到一个 Historical Change 的表中，能够通过它提供的函数查询，也可以设置定期清理。相当于记录变更历史、自己手动清理陈旧数据这些工作，由 SQLServer 帮你做了。但是不开源咱也不知道它具体怎么实现的。

大致的背景了解到这里，我们再回到 F1 上。论文里提到，一般 Change Tracking 的实现方式，要么是利用数据库触发器，要么是在应用程序中自行实现记录历史变更的逻辑（比如 AdWords 之前采用的实现方式就是通过 Java 客户端执行事务时，自行记录修改并写入数据库）；总之都比较繁琐。而 F1 中优先考虑了这一特性（其实通过 Spanner 的 MVCC 蛮好实现的）。大概思路是：F1 执行事务时，为事务生成 *ChangeBatch Protocol Buffer*，记录修改数据的版本，并作为被修改 table 的 child table 存储起来。*ChangeBatch* 的 key 由原始数据的 primary key 和修改时的 timestamp 组成。

这样设计 *ChangeBatch* 的 key 使所有修改记录会按照事务提交的时间顺序来排序，在查找时非常便利。F1 提供的 Change History 特性可以结合 Publish-and-Subscribe 机制来将更新增量推送给应用程序。

> 文中还提到另一个应用是可以用于缓存，比方说客户端在读取网页数据时，在本地内存中缓存了网页内容；如果客户端提交了一些事务对数据进行了修改，那么它的缓存也应当更新。我们可以采取这样的方式来更新缓存：当客户端通过 row key 和 timestamp（希望看到的事务对应的时间戳） 访问对应数据项的缓存，发现缓存过期了，则读取当前已缓存版本之后的全部修改，作为新的缓存数据。文中说这样的缓存更新机制更加准确，比起全部替换成本也更低。

[^6-1]: [知乎 - 知乎问题的修改记录的数据库是怎样设计的？][6.1]
[^6-2]: [SQLServer - Track Data Changes](https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/track-data-changes-sql-server?view=sql-server-ver15)


[6.1]: https://www.zhihu.com/question/19814158
------------------

sec 7: Client Design

7.1 Simplfied ORM

ORM 的缺点：归纳来说是扩展性太差，如果用来访问 F1 这样大规模、且延迟本身就高的数据库，性能极差。因此 F1 提供了新的 ORM 层 API，


原本的 ORM API 在较大数据集上运行时，平均延迟会从原本的 200～300ms 迅速增长，最慢的甚至会需要数秒才能完成；新的 F1 ORM 虽然最短的延迟比起之前略微增加，但是平均延迟并没有增加太多，最高的延迟也仅是中位数的几倍。

// TODO：这部分其实没什么用，没介绍太多实质性内容

7.2 SQL Interface

sec 8： Query Processing

properties：

1. Query 可以中心化的执行（低延迟），也可以并行执行。
  Central for OLTP on one server node
  Distributed for OLAP

2. Batching->降低网络延迟的影响
3. hash-based repartition
4. 独立的 query plan operator，


分布式的 Query 是否主要体现在 Join 操作上？ 

传统数据库在执行查询是，通常是从本地取数据来进行查询处理的；但 F1 的数据放在 Spanner 上，由于 Spanner 分布式存储的特点（加上底层 CFS），数据可能位于许多物理节点上， 

传统数据库中是 Disk Latency，在 F1 中是 Network Latency

传统数据库执行查询主要的性能瓶颈是磁盘的读写速度，因此传统数据库会对查询进行优化，尽量减少磁盘 I/O 的次数; F1 中相同的策略并不适用，F1 的架构让它可以通过 batching 的方式，即通过批量发送读取数据请求，来获得更高的处理速度。

简单理解就是，F1 可以尽可能增加一次批量读取的请求数量，发送给一大批物理节点，直到具体分配到每个节点上的请求数量，能使该节点满载工作。在这个过程中 F1 都是能够获得性能提升的，而不用像传统数据库那样考虑优化查询请求的 I/O 数量。


sec 8.4 疑问：

// TODO： repartition 是什么，怎么做？ co-partition？

需要聚合数据时，如果数据太大则将其分割放到硬盘中，逐块处理。

分布式的 hash join/hash aggregation，如果部分节点宕机，意味着整个 query 应当重新执行。但是 F1 的 pipeline 应当如何判断有节点故障了？如何判断 query 没有获取全部结果？

应该说，故障节点上一层的执行节点会发现故障的出现，因此不会继续执行；同层的应该就无法感知，因此还是会继续执行。

无 checkpoint，因此如果 query 执行过程中任意参与的节点故障则整个 query 需要重新执行。

8.6 疑问：
如果单个 query coordinator 以及单个客户端读取数据是性能瓶颈，但如何使用多个 endpoints？它们怎么获取最终的结果？比如需要排序的查询，或者需要聚合的查询，如何分布到多个 coordinator？还是说这样的确实不能分，能分的是那些能够拆分的查询，比如不要求排序的、只是单纯列出过去一年内的点击数据等。这样就可以分到多个 coordinator 中以 pipeline 形式返回数据。

这种方式有个问题是 F1 是以 lock-step 的方式向 reader 返回数据，因此一个 slow reader 就会拖慢整个查询。论文提供一种优化思路是，// 没看懂，主要对本来的 parallel 过程就没完全搞懂。

8.7 

repeated fields

Proto Join

9 Delpoyment
东西海岸各自 2 节点，中部 1 节点，


10 

总结来说，F1 比起谷歌之前采用的 MySQL 分片的存储方式，查询的平均延迟并没有增加，但是可扩展性大大提高了。

// MySQL sharding 的局限性

并且 tail latency 降低了（延迟的最大值降低了）；得益于 F1 的架构，分布式查询的速度也有所提升，因为原本的查询速度受限于 MySQL 分片的数量，而 MySQL 分片增加又会导致 /// TODO 等问题，所以需要谨慎考虑分片数量，//，现在可以在原本的 5个 Paxos replica 之外，增加额外的 Read-only replica 来提高查询速度。

不过 F1 也存在缺点，会比 MySQL 使用更多的 CPU 资源。


在我理解看来，F1 的工作（或者说编码）主要在 Query Execution、Transaction 部分； Spanner 是被作为 F1 的存储引擎来使用的，虽然 Spanner 也有提供一些事务的特性和 SQL 支持。