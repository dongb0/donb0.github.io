---
layout: post
title: "[db/paper] 0 - GFS"
subtitle: 
author: "Dongbo"
catalog: true
header-style: text
tags:
  - paper reading
  - distributed system
  - database
---

> 表面上post时间是2019-11-09，但实际写作时间是2023-11-19

### 序

好，那么这一次我们来看GFS论文。

从上面展示的咕咕时间对比可以看出，尽管很早之前就想要了解分布式系统在工业界首次应用的典型，尽管也确实花了点时间读了论文和众多的解读资料，但是始终没有~~（时间）~~信心做自己的解读和归纳；今天终于决定尝试这么做的动机是，上周看到Oceanbase 1.0架构的文档，从组件命名中发现 1.0 之前的OB是参考了GFS的架构：比如ChunkServer、单点的UpdateServer（GFS中的master）。因此决定回头捡一下相关的知识，这次就不再逐段翻译了，只抽取一些重点进行解读。

首先从摘要我们能了解到的信息有：GFS是一个能提供 fault tolerant 的分布式文件系统，并且是通过数百数千个普通服务器节点组件起来的，具有很高的可扩展性。这一点是非常有开创意义的，这意味着我们可以不断增加节点来提高集群。// 但是当时的GFS是否还是单点更新的？先往后看。

Intro部分介绍了GFS设计时需要考虑的几个因素，大部分都是面向谷歌的具体应用场景，但其中的第一点是至今仍然通用的：认为集群中有节点宕机/故障是经常出现的情况。这意味着系统需要将节点的故障和恢复妥善处理好，使得整体服务不受个别节点的故障影响。GFS具体如何做到这一点，是我们今天关注的重点。

> 顺带一提，在传统数据库架构中，容错通常是通过主备节点的方式来完成，比如Oracle/MySQL // TODO：replica，// 主备节点之间通过日志同步的方式来实现数据同步，这种架构是不能将节点故障作为常态处理的。比如在最大保护模式下，备节点的故障也会使得主节点停止写入；最大性能模式下主节点故障，由于日志同步是异步的，切换到备节点可能出现丢数据的情况，均无法做到无缝切换节点提供服务，无法将节点故障看作“常态”（不如说这些架构是基于“节点不太可能故障”来设计的，比如之前Oracle通常运行在昂贵的小型机上//几百万RMB一台）

GFS提供了类似一般文件系统的接口，保证多个客户端并发的写请求可以原子完成（无锁）；支持快照功能，能够低开销地创建文件或目录的快照。这些功能中，我们关注GFS如何实现无锁的并发追加写，以及如何创建快照。

> 虽然支持随机写，但 GFS 设计面向的负载为追加写（append）居多

### GFS 架构

// 是否需要贴架构图？

GFS 由一个 master 和若干 chunkserver 构成。

master 上存储各种meta信息，包含三种：1.namespace，2.文件名到chunk的映射关系（比如file1存储在chunk 1、3、4中），3.chunk的位置等（比如chunk1 在 server 10.11.23.1、chunk 4在 10.11.23.4上），前二者需要持久化到日志，chunk的位置不持久化，而是通过 master 轮询所有 chunkserver 来获取；master 也负责维护chunk的租约、迁移以及回收等工作；

> 文中提到不持久化 chunk 的位置而是通过定期轮询的方式，能够省略每次节点写入后、重启后、改名后都与 master 进行同步来更新信息。总而言之，降低了系统的复杂性，但代价是 master 上信息更新的延迟相对较高，若对已经变动的 chunkserver 发送读写请求，就需要等待 master 更新最新的 chunkserver 信息后才能继续；不过 GFS 的设计目标之一就是优先追求高带宽而不注重请求的延迟，因此他们可以采用这种方式。

chunkserver 则将 chunk 作为普通的linux文件存储在本地，同时每个chunk也设置了多份备份作为容错。GFS 保证 chunkserver 上存储的文件块是一致的 /\*TODO：这里的一致性是什么程度的保证？\*/ ，因此读取操作读任意一个副本即可；写入操作为了保证所有副本的一致，GFS会由master挑选出某个 replica 作为 primary 来组织写入操作的顺序。下文将对读写操作展开进行详细描述。

此外，虽然GFS使用了 master 这样的单点组件，但是 master 同样是有多个副本来避免单点故障的。

// 具体master多副本的使用方法是：好像并没有？只是日志做了多个备份。

#### 读取

// 简单就贴个图更清晰一些

读操作比较简单，大致流程为：client向 master 发送需要读的chunk和index，master返回对应 chunkserver的位置信息；随后 client 直接与 chunkserver 交互完成读取。同时client还可以缓存 chunkserver 的 meta 信息，在租约过期前不用再向 master 发送请求，进一步减少 master 的负载。  

#### 写入

master 通过给 namespace 加锁保证写入操作的原子性；通过 master 的日志保证加锁操作的全局顺序。

master 给其中一个 replica 授予租约（通常 60s）作为 primary，由 primary 组织写入操作的顺序，其他replica 按照这个顺序应用（raft！）。

写入流程为：

client 向 master 询问持有租约的chunkserver，master 返回 primary 和 其他 secondary replica 的信息。

client 向所有 replica 推送数据；replica 收到后会缓存在 LRU buffer，全部 ack 后，clint 再向 primary 发送写入请求。primary 为写入操作定序（可能就是按照它收到的请求的顺序，虽然可能来自许多客户端的并发请求），并将修改应用到本地。

primary 将写请求转发给 secondary，secondary 按照 primary 指定的顺序应用；完成后向 primary 返回 ack。

primary 收到所有 ack 后向 client 返回成功。如果写入 secondary 发生错误，则认为这次写入失败，这部分文件处于不一致状态。客户端会重试这次写入操作。
// 不一致的文件块 如何处理？等待GC？
> primary fail 那么当作本次写入没有发生。primary 写入的完成相当于事务的提交点。


### 总结

回答文章开头提出的几个问题：

1. GFS如何应对节点故障、使得系统在集群节点发生故障的情况下依然提供正常服务的？

2. GFS如何实现无锁的并发写入？

3. 快照