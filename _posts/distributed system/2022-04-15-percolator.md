---
layout: post
title: "[分布式事务] 1 - Percolator"
subtitle: 
author: "Dongbo"
header-style: text
mathjax: true
hidden: false
catalog: true
tags:
  - distributed system
---

好，今天我们来看 Percolator。

不知道为什么检索了许多有关分布式事务的博客，很少有介绍分布式事务提交协议时带上 Percolator 的。可能因为 Percolator 是基于 BigTable、针对增量更新操作事务设计的，不太适合其他分布式事务的场景。虽然有点好奇为何 TiKV 选择了这样一个提交协议，但是我们还是先看 Percolator 的工作原理，然后才好分析它的优缺点，以及 TiKV 在此基础上做了什么改动[^1],[^2]。

### 引

前面我们也说了，Percolator 本身是谷歌为了提高他们网页索引更新的效率而设计的，这一场景主要特点有数据规模大（PB级别）、数据分布在数千个节点中（使用BigTable存储）、实时性需求低（用户会晚一些看到搜索结果更新，这影响并不大）、数据更新的部分相对较小。

// TODO：BigTable大概思想

论文中其实也提到，可以实现一个不支持事务的增量更新系统，但是有事务的ACID性质有助于判断系统应处于什么状态，有利于保持索引表的一致性和 up to date。我们本身就是想介绍这是怎样一个分布式提交协议、如何支持分布式事务的性质，因此就不花太多笔墨介绍为何需要分布式事务了，来看重点吧。

Percolator 是采用一种两阶段提交的方式来进行事务的提交，且在 BigTable 中存储多个版本的数据（基于时间戳的MVCC）来提供 Snapshot Isolation 的隔离级别。但它的提交与之前所说的 2PC 过程还是有挺大区别的，而且使用时间戳的同时还用了锁来进行并发控制这一点也让人费解，下面我们一点点来介绍。不过我这里的描述也仅是一个分布式领域萌新的一家之言，如果理解有误也请大家批评指正。 // TODO：评论区

### Percolator 提交过程

| key   | bal:data | bal:lock |bal:write |
|-------|----------|----------|----------|
|Bob   | 6:<br>5:$10    | 6:<br>5:     | 6:data@5<br>5:    |
|Joe   | 6:<br>5:$2     | 6:<br>5:     | 6:data@5<br>5:    |

这是来自论文里的示例，一条记录除了原本的k/v字段之外，还增加了 lock 和 write 两个字段用于并发控制；除此之外其实还有 norify 和 ack_O 字段用于 Notification，但展示 lock 和 write 就足够我们理解 Percolator 工作过程，这里我们暂且不提。

- data 字段是实际存储 value 的地方；
- lock 字段用于表示对某个key加锁，key 的 lock 字段存在值时表示该 key 被锁上，释放锁时清空该字段的值。之所以用一个字段来表示锁，论文是这样考虑的：Percolator 是作为 client 访问 BigTable，无法直接控制 BigTable 对某部分数据加锁，因而相当于需要自己在应用层面实现 lock manager。对这样一个组件的要求是：锁的信息需要持久化、高吞吐量、分布式且能够负载均衡。BigTable 足够满足以上需求，于是他们合计直接将锁存在 BigTable 中，也就是现在这样用一个字段标识锁的做法。
- write 字段是用来标识事务是否完成提交的。如果事务成功提交，则会在 write 字段增加一行数据，内容为本次提交事务对应的版本号。

> 关于为什么需要锁的一些分析： 参考数据库系统概念第六版 15.4.2 时间排序协议小节最后保证可恢复调度部分、15.6.1 多版本时间戳排序，以及习题15.30，时间戳排序协议为了防止不可恢复调度的出现（比如以下的调度在时间戳排序中允许出现，但是却是不可恢复的），可以采用以下方法：1）使用lock 将读取未提交数据的操作推迟到更新该数据的事务提交之后；2）使用一个 commit bit 表示事务正在修改某数据项，用于推迟读操作。  
使用多版本时间戳排序也不能保证可恢复性，如果要保证调度可恢复，依然需要采用上述改动中的一种。但是这样其实损失了 MVCC 原本可以立即响应读操作的优势。

|T1   |T2   |
|--------|--------|
|read(A) |        |
|write(A)|        |
|        |read(A) |
|        |commit  |
|read(B) |        |

因此 Percolator 的两阶段提交过程如下：

- PreWrite 阶段尝试为所有要修改的数据申请锁，然后检查是否存在以下冲突：

  - write 字段的时间戳大于当前事务，说明有其他事务先于本事务修改了该数据并完成提交，为了防止更新丢失，本事务需要回滚；
  - 某个数据上的锁已存在，说明其他事务在修改该数据，本事务回滚。

- 若不存在冲突则进入 Commit 阶段，事务清除 lock 字段中存储的锁，然后在 write 字段中记录事务的开始时间戳，完成提交过程。

需要注意的是如小节开头表格中展示的那样，每个字段写入时都要附带一个写入时间戳，在 Percolator 中由一个 timestamp oracle 来进行时间戳的分配。

同时读操作也如上所说，读事务查看[0, start_timestamp]时间范围内的数据版本，然后等待写事务释放数据的锁（如果有的话），然后才开始读最新版本的数据。暂时理解为这是为了事务的可恢复而牺牲了读操作的并发度吧。

Percolator 并不像2PC中有一个集中式的协调器（这样的架构肯定没法扩展到谷歌需要的数千节点集群），而是由发起事务的客户端来进行事务的提交操作，比方说通过一个集群在执行爬虫任务，那么集群中的任何一个节点都能向 Percolator 发起事务的提交请求，从而避免了 2PC 中协调器单点故障的问题。不过由于客户端存在故障的可能，提交操作可能会卡在某个中间状态， Percolator 还需要处理这些异常状态。

客户端故障可能会造成事务未完成提交或事务提交后没有来得及清理锁，因此一个新事务执行时，如果遇到某个字段上有锁，需要判断给该字段加锁的 worker 是否还在正常执行，是否需要回滚或重做；否则如果按照前面描述的两阶段提交过程直接回滚的话，一个异常终止的事务就会阻塞后续所有事务的执行。

Percolator 采用的方法是在一个事务中选取一个 key 作为 primary，以 primary 的提交作为事务的提交点。如果新事务扫描到异常终止的事务留下来的锁，可以通过 primary 对应的 lock 字段判断这个事务是否进入提交状态：如果 primary lock 不存在，而对应的 write 字段已有更新，则该事务已进入提交，需要重做其它未完成的修改；如果 primary lock 存在，则事务还未提交，可以进行回滚。

不过到这里我们还是无法判断碰到一个 key 被锁住时，究竟是因为 worker 故障了，还是因为事务正在执行。如果我们每次碰到 key 被锁住时，都回滚之前事务的操作，很容易干扰正常事务的执行。Percolator 解决这一问题的策略是将 worker 状态存到 chubby 中来判断发起某个事务的 wroker 是否存活；只有检测到对应 worker 宕机之后才会按照上述方式清理 lock 并继续新事务的修改过程。也就是使用 primary 其实并不能告诉我们事务状态是否异常，且如果节点故障导致事务提交失败，重启后节点本身也并不会进行回滚操作，而是下一次事务开始提交时（可能是故障节点自己重启后进行，也可能使其它节点提交的事务），才将对应的 meta 给清理掉。其实清理工作中主要的部分是清除 lock 字段，未提交的 data 虽然存在但是由于没有对应的 write 字段，是不会被其他事务读取到的（也可以顺带将没有对应 write 的 data 部分清除来避免空间的浪费）。另外 Percolator 还给每个锁加上了有效期，失效的锁可以被直接清除；如果事务还在正常执行过程中，那么需要在锁过期之前更新它的有效时间，防止被直接清除。

以上就是 Percolator 提交的大概思想了，其他的细节比如时间戳的生成、Notification机制等下次再考虑是否展开介绍一下吧。

// TODO：notify  // TODO：timestamp 考虑是否需要

The End

----------------


[^1]: [Percolator原始论文:Large-scale Incremental Processing Using Distributed Transactions and Notifications](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36726.pdf)
[^2]: [TiKV 中 Percolator 的优化](https://tikv.org/deep-dive/distributed-transaction/optimized-percolator/)
